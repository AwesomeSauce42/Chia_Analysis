{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c179485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda76ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d849e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = f\"data/Chia_with_Scope\"\n",
    "outputpath = f\"data/Chia_to_bio\"\n",
    "trainpath = f\"data/Train\"\n",
    "testpath = f\"data/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc95b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfiles = set()\n",
    "for f in os.listdir(inputpath):\n",
    "    if f.endswith('.ann'):\n",
    "        inputfiles.add(f.split('.')[0].split('_')[0])\n",
    "len(inputfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d3c055f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of entity types to retain\n",
    "select_types = ['Condition', 'Value', 'Drug', 'Procedure', 'Measurement', 'Temporal', \\\n",
    "    'Observation', 'Person', 'Mood', 'Device', 'Pregnancy_considerations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "767b572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Brat format into BIO format\n",
    "# function for getting entity annotations from the annotation file\n",
    "def get_annotation_entities(ann_file, select_types=None):\n",
    "    entities = []\n",
    "    with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                term = line.strip().split('\\t')[1].split()\n",
    "                if (select_types != None) and (term[0] not in select_types): continue\n",
    "                if int(term[-1]) <= int(term[1]): continue\n",
    "                entities.append((int(term[1]), int(term[-1]), term[0]))\n",
    "    return sorted(entities, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "# function for handling overlap by keeping the entity with largest text span\n",
    "def remove_overlap_entities(sorted_entities):\n",
    "    keep_entities = []\n",
    "    for idx, entity in enumerate(sorted_entities):\n",
    "        if idx == 0:\n",
    "            keep_entities.append(entity)\n",
    "            last_keep = entity\n",
    "            continue\n",
    "        if entity[0] < last_keep[1]:\n",
    "            if entity[1]-entity[0] > last_keep[1]-last_keep[0]:\n",
    "                last_keep = entity\n",
    "                keep_entities[-1] = last_keep\n",
    "        elif entity[0] == last_keep[1]:\n",
    "            last_keep = (last_keep[0], entity[1], last_keep[-1])\n",
    "            keep_entities[-1] = last_keep\n",
    "        else:\n",
    "            last_keep = entity\n",
    "            keep_entities.append(entity)\n",
    "    return keep_entities\n",
    "\n",
    "# inverse index of entity annotations\n",
    "def entity_dictionary(keep_entities, txt_file):\n",
    "    f_ann = {}\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f: # Marked\n",
    "        text = f.readlines()\n",
    "        if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']:\n",
    "            text = ' '.join([i.strip() for i in text])\n",
    "        else:\n",
    "            text = '  '.join([i.strip() for i in text])\n",
    "    for entity in keep_entities:\n",
    "        entity_text = text[entity[0]:entity[1]]\n",
    "        doc = nlp(entity_text)\n",
    "        token_starts = [(i, doc[i:].start_char) for i in range(len(doc))]\n",
    "        term_type = entity[-1]\n",
    "        term_offset = entity[0]\n",
    "        for i, token in enumerate(doc):\n",
    "            ann_offset = token_starts[i][1]+term_offset\n",
    "            if ann_offset not in f_ann:\n",
    "                f_ann[ann_offset] = [i, token.text, term_type]\n",
    "    return f_ann\n",
    "\n",
    "# Brat -> BIO format conversion\n",
    "for infile in inputfiles:\n",
    "    for t in [\"exc\", \"inc\"]:\n",
    "        file = f\"{infile}_{t}\"\n",
    "        ann_file = f\"{inputpath}/{file}.ann\"\n",
    "        txt_file = f\"{inputpath}/{file}.txt\"\n",
    "        out_file = f\"{outputpath}/{file}.bio.txt\"\n",
    "        sorted_entities = get_annotation_entities(ann_file, select_types)\n",
    "        keep_entities = remove_overlap_entities(sorted_entities)\n",
    "        f_ann = entity_dictionary(keep_entities, txt_file)\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                sent_offset = 0\n",
    "                for line in f:\n",
    "                    # print(line.strip())\n",
    "                    if '⁄' in line:\n",
    "                        # print(txt_file)\n",
    "                        line = line.replace('⁄', '/') # replace non unicode characters\n",
    "                    doc = nlp(line.strip())\n",
    "                    token_starts = [(i, doc[i:].start_char) for i in range(len(doc))]\n",
    "                    for token in doc:\n",
    "                        token_sent_offset = token_starts[token.i][1]\n",
    "                        token_doc_offset = token_starts[token.i][1]+sent_offset\n",
    "                        if token_doc_offset in f_ann:\n",
    "                            if f_ann[token_doc_offset][0] == 0:\n",
    "                                label = f\"B-{f_ann[token_doc_offset][2]}\"\n",
    "                            else:\n",
    "                                label = f\"I-{f_ann[token_doc_offset][2]}\"\n",
    "                        else:\n",
    "                            label = f\"O\"\n",
    "                        # print(token.text, token_sent_offset, token_sent_offset+len(token.text), token_doc_offset, token_doc_offset+len(token.text), label)\n",
    "                        f_out.write(f\"{token.text} {token_sent_offset} {token_sent_offset+len(token.text)} {token_doc_offset} {token_doc_offset+len(token.text)} {label}\\n\")\n",
    "                    # print('\\n')\n",
    "                    f_out.write('\\n')\n",
    "                    if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']: # 3 trials with inconsistent offsets\n",
    "                        sent_offset += (len(line.strip())+1)\n",
    "                    else:\n",
    "                        sent_offset += (len(line.strip())+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce84481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 100 100\n"
     ]
    }
   ],
   "source": [
    "# dataset separation: 800 trials (80%) for training, 100 trials (10%) for validation and 100 trials (10%) for testing\n",
    "train_ids, dev_ids = train_test_split(list(inputfiles), train_size=0.8, random_state=13, shuffle=True)\n",
    "dev_ids, test_ids = train_test_split(dev_ids, train_size=0.5, random_state=13, shuffle=True)\n",
    "print(len(train_ids), len(dev_ids), len(test_ids))\n",
    "chia_datasets = {\"train\":train_ids, \"dev\":dev_ids, \"test\":test_ids}\n",
    "json.dump(chia_datasets, open(\"data/chia_datasets.json\", \"w\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "575fddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge BIO format train, validation and test datasets\n",
    "# chia_datasets = json.load(open(\"chia/chia_datasets.json\", \"r\", encoding=\"utf-8\"))\n",
    "# merge the train dataset\n",
    "with open(\"data/train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"train\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{trainpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{trainpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# merge the validation dataset\n",
    "with open(\"data/dev.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"dev\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{trainpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{trainpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# merge the test dataset\n",
    "with open(\"data/test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"test\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{testpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{testpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d5b9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Chia in Brat into format for Att-BiLSTM-CRF model\n",
    "out_file = f\"data/chia_ner.tsv\"\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for infile in inputfiles:\n",
    "        for t in [\"exc\", \"inc\"]:\n",
    "            file = f\"{infile}_{t}\"\n",
    "            ann_file = f\"{inputpath}/{file}.ann\"\n",
    "            txt_file = f\"{inputpath}/{file}.txt\"\n",
    "            sorted_entities = get_annotation_entities(ann_file, select_types)\n",
    "            keep_entities = remove_overlap_entities(sorted_entities)\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                sent_offset = 0\n",
    "                for line in f:\n",
    "                    # print(line.strip())\n",
    "                    if '⁄' in line: line = line.replace('⁄', '/')\n",
    "                    sent_end = sent_offset + len(line)\n",
    "                    sent_ents = []\n",
    "                    for ent in keep_entities:\n",
    "                        if ent[0] < sent_offset or ent[1] < sent_offset: continue\n",
    "                        if ent[0] >= sent_end or ent[1] > sent_offset+len(line.strip()): break\n",
    "                        ent_start = ent[0]-sent_offset+1\n",
    "                        ent_end = ent[1]-sent_offset+1\n",
    "                        sent_ents.append(f\"{ent_start}:{ent_end}:{ent[2].lower()}\")\n",
    "                    if sent_ents == []:\n",
    "                        if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']:\n",
    "                            sent_offset += (len(line.strip())+1)\n",
    "                        else:\n",
    "                            sent_offset += (len(line.strip())+2)\n",
    "                        continue\n",
    "                    # print(f\"{file}\\t{','.join(sent_ents)}\\t{line.strip()}\")\n",
    "                    f_out.write(f\"{file}\\t{','.join(sent_ents)}\\t{line.strip()}\")\n",
    "                    # print('\\n')\n",
    "                    f_out.write('\\n')\n",
    "                    if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']:\n",
    "                        sent_offset += (len(line.strip())+1)\n",
    "                    else:\n",
    "                        sent_offset += (len(line.strip())+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1360e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split Chia in format for Att-BiLSTM-CRF model into train, validation and test datasets\n",
    "# chia_datasets = json.load(open(\"chia/chia_datasets.json\", \"r\", encoding=\"utf-8\"))\n",
    "with open(\"data/chia_ner_train.tsv\", \"w\", encoding=\"utf-8\") as ftrain, open(\"data/chia_ner_dev.tsv\", \"w\", encoding=\"utf-8\") as fdev, open(\"data/chia_ner_test.tsv\", \"w\", encoding=\"utf-8\") as ftest:\n",
    "    with open(\"data/chia_ner.tsv\", \"r\", encoding=\"utf-8\") as fread:\n",
    "        for line in fread:\n",
    "            if line.split('\\t', 1)[0].split(\"_\")[0] in chia_datasets[\"train\"]:\n",
    "                ftrain.write(line)\n",
    "            elif line.split('\\t', 1)[0].split(\"_\")[0] in chia_datasets[\"dev\"]:\n",
    "                fdev.write(line)\n",
    "            else:\n",
    "                ftest.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "405c4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc_strict_match(gs, pred, s_idx, e_idx, ent_type):\n",
    "    if s_idx == 0:\n",
    "        for idx in range(s_idx, e_idx):\n",
    "            if gs[idx] != pred[idx]:\n",
    "                return False\n",
    "        if e_idx < len(gs):\n",
    "            if gs[e_idx] == ent_type or pred[e_idx] == ent_type:\n",
    "                return False\n",
    "    else:\n",
    "        if gs[s_idx-1] == ent_type or pred[s_idx-1] == ent_type:\n",
    "            return False\n",
    "        for idx in range(s_idx, e_idx):\n",
    "            if gs[idx] != pred[idx]:\n",
    "                return False\n",
    "        if e_idx < len(gs):\n",
    "            if gs[e_idx] == ent_type or pred[e_idx] == ent_type:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def abc_relax_match(gs, pred, s_idx, e_idx, ent_type):\n",
    "    for idx in range(s_idx, e_idx):\n",
    "        if gs[idx] == pred[idx] == ent_type:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b051f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'chia' # or 'frd', 'chia'\n",
    "outfolder = f\"data/attbilstmcrf\"\n",
    "outfile = f\"{dataset}_attbilstmcrf_results.txt\"\n",
    "labels_dict = {'chia':['Mood', 'Condition', 'Procedure', 'Measurement', 'Value', 'Drug', 'Temporal', 'Observation', 'Pregnancy', 'Person', 'Device'], 'frd':['chronic_disease', 'treatment', 'upper_bound', 'pregnancy', 'clinical_variable', 'lower_bound', 'cancer', 'age', 'language_fluency', 'gender', 'contraception_consent', 'technology_access', 'allergy_name', 'bmi', 'ethnicity']}\n",
    "labels = labels_dict[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7309c695",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m:{},\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m:{}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:{}}\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m         gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_metrics = {\"category\":{},\"overall\":{}, \"prediction\":{}}\n",
    "with open(f\"{outfolder}/{outfile}\", \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        gs = eval(line.split('\\t')[1])\n",
    "        pred = eval(line.split('\\t')[0])\n",
    "        print(gs, pred)\n",
    "        for i in zip(gs, pred):\n",
    "            if i[0] == i[1]: eval_metrics[\"overall\"][\"acc_true\"] = eval_metrics[\"overall\"].get(\"acc_true\", 0) + 1\n",
    "            else: eval_metrics[\"overall\"][\"acc_false\"] = eval_metrics[\"overall\"].get(\"acc_false\", 0) + 1\n",
    "        llen = len(gs)\n",
    "        cur_idx = 0\n",
    "        while cur_idx < llen:\n",
    "            if gs[cur_idx] == 0:\n",
    "                cur_idx += 1\n",
    "            else:\n",
    "                start_idx = cur_idx\n",
    "                end_idx = start_idx + 1\n",
    "                cate = gs[start_idx]\n",
    "                while end_idx < llen and gs[end_idx] == cate:\n",
    "                    end_idx += 1\n",
    "                eval_metrics[\"overall\"]['gs'] = eval_metrics[\"overall\"].get('gs', {})\n",
    "                eval_metrics[\"overall\"]['gs']['count'] = eval_metrics[\"overall\"]['gs'].get('count', 0) + 1\n",
    "                eval_metrics[\"overall\"]['gs'][labels[cate-1]] = eval_metrics[\"overall\"]['gs'].get(labels[cate-1], 0) + 1\n",
    "                if abc_strict_match(gs, pred, start_idx, end_idx, cate):\n",
    "                    eval_metrics[\"overall\"][\"strict_predicted\"] = eval_metrics[\"overall\"].get(\"strict_predicted\", 0) + 1\n",
    "                    eval_metrics[\"category\"][labels[cate-1]] = eval_metrics[\"category\"].get(labels[cate-1], {\"strict\":0, \"relax\":0, \"miss\":0})\n",
    "                    eval_metrics[\"category\"][labels[cate-1]][\"strict\"] += 1\n",
    "                elif abc_relax_match(gs, pred, start_idx, end_idx, cate):\n",
    "                    eval_metrics[\"overall\"][\"relax_predicted\"] = eval_metrics[\"overall\"].get(\"relax_predicted\", 0) + 1\n",
    "                    eval_metrics[\"category\"][labels[cate-1]] = eval_metrics[\"category\"].get(labels[cate-1], {\"strict\":0, \"relax\":0, \"miss\":0})\n",
    "                    eval_metrics[\"category\"][labels[cate-1]][\"relax\"] += 1\n",
    "                else:\n",
    "                    eval_metrics[\"overall\"][\"miss_predicted\"] = eval_metrics[\"overall\"].get(\"miss_predicted\", 0) + 1\n",
    "                    eval_metrics[\"category\"][labels[cate-1]] = eval_metrics[\"category\"].get(labels[cate-1], {\"strict\":0, \"relax\":0, \"miss\":0})\n",
    "                    eval_metrics[\"category\"][labels[cate-1]][\"miss\"] += 1\n",
    "                cur_idx = end_idx\n",
    "        cur_idx = 0\n",
    "        while cur_idx < llen:\n",
    "            if pred[cur_idx] == 0:\n",
    "                cur_idx += 1\n",
    "            else:\n",
    "                start_idx = cur_idx\n",
    "                end_idx = start_idx + 1\n",
    "                cate = pred[start_idx]\n",
    "                while end_idx < llen and pred[end_idx] == cate:\n",
    "                    end_idx += 1\n",
    "                if abc_strict_match(gs, pred, start_idx, end_idx, cate):\n",
    "                    eval_metrics[\"overall\"][\"strict_predict\"] = eval_metrics[\"overall\"].get(\"strict_predict\", 0) + 1\n",
    "                    eval_metrics[\"prediction\"][labels[cate-1]] = eval_metrics[\"prediction\"].get(labels[cate-1], {\"strict\":0, \"relax\":0, \"miss\":0})\n",
    "                    eval_metrics[\"prediction\"][labels[cate-1]][\"strict\"] += 1\n",
    "                elif abc_relax_match(gs, pred, start_idx, end_idx, cate):\n",
    "                    eval_metrics[\"overall\"][\"relax_predict\"] = eval_metrics[\"overall\"].get(\"relax_predict\", 0) + 1\n",
    "                    eval_metrics[\"prediction\"][labels[cate-1]] = eval_metrics[\"prediction\"].get(labels[cate-1], {\"strict\":0, \"relax\":0, \"miss\":0})\n",
    "                    eval_metrics[\"prediction\"][labels[cate-1]][\"relax\"] += 1\n",
    "                else:\n",
    "                    eval_metrics[\"overall\"][\"miss_predict\"] = eval_metrics[\"overall\"].get(\"miss_predict\", 0) + 1\n",
    "                    eval_metrics[\"prediction\"][labels[cate-1]] = eval_metrics[\"prediction\"].get(labels[cate-1], {\"strict\":0, \"relax\":0, \"miss\":0})\n",
    "                    eval_metrics[\"prediction\"][labels[cate-1]][\"miss\"] += 1\n",
    "                cur_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88145043",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m attbc_metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m:{},\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m:{}}\n\u001b[1;32m----> 2\u001b[0m attbc_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43meval_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43macc_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m/\u001b[39m(eval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_true\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39meval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_false\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m pred_all \u001b[38;5;241m=\u001b[39m eval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict_predict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m eval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelax_predict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m eval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiss_predict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m pre_relax_all \u001b[38;5;241m=\u001b[39m (eval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict_predict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m eval_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelax_predict\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m/\u001b[39m pred_all\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc_true'"
     ]
    }
   ],
   "source": [
    "attbc_metrics = {\"category\":{},\"overall\":{}}\n",
    "attbc_metrics[\"overall\"][\"acc\"] = eval_metrics[\"overall\"][\"acc_true\"]/(eval_metrics[\"overall\"][\"acc_true\"]+eval_metrics[\"overall\"][\"acc_false\"])\n",
    "pred_all = eval_metrics[\"overall\"]['strict_predict'] + eval_metrics[\"overall\"]['relax_predict'] + eval_metrics[\"overall\"]['miss_predict']\n",
    "pre_relax_all = (eval_metrics[\"overall\"]['strict_predict'] + eval_metrics[\"overall\"]['relax_predict'])/ pred_all\n",
    "rec_relax_all = (eval_metrics[\"overall\"]['strict_predicted'] + eval_metrics[\"overall\"]['relax_predicted'])/ eval_metrics[\"overall\"]['gs']['count']\n",
    "f1_relax_all = (2*pre_relax_all*rec_relax_all)/(pre_relax_all+rec_relax_all)\n",
    "print(f\"Overall Relax Level: Precision: {pre_relax_all}, Recall: {rec_relax_all}, F1: {f1_relax_all}\")\n",
    "attbc_metrics[\"overall\"][\"relax\"] = {\"f_score\": f1_relax_all, \"precision\":pre_relax_all, \"recall\":rec_relax_all}\n",
    "\n",
    "pre_strict_all = eval_metrics[\"overall\"]['strict_predict'] / pred_all\n",
    "rec_strict_all = eval_metrics[\"overall\"]['strict_predicted'] / eval_metrics[\"overall\"]['gs']['count']\n",
    "f1_strict_all = (2*pre_strict_all*rec_strict_all)/(pre_strict_all+rec_strict_all)\n",
    "print(f\"Overall Strict Level: Precision: {pre_strict_all}, Recall: {rec_strict_all}, F1: {f1_strict_all}\")\n",
    "print('\\n')\n",
    "attbc_metrics[\"overall\"][\"strict\"] = {\"f_score\": f1_strict_all, \"precision\":pre_strict_all, \"recall\":rec_strict_all}\n",
    "\n",
    "for i in eval_metrics[\"category\"].keys():\n",
    "    tt = eval_metrics[\"overall\"]['gs'][i]\n",
    "    tp = eval_metrics[\"prediction\"][i]['strict'] + eval_metrics[\"prediction\"][i]['relax'] + eval_metrics[\"prediction\"][i]['miss']\n",
    "    \n",
    "    pre_relax = (eval_metrics[\"prediction\"][i]['strict']+eval_metrics[\"prediction\"][i]['relax'])/tp\n",
    "    rec_relax = (eval_metrics[\"category\"][i]['strict']+eval_metrics[\"category\"][i]['relax'])/tt\n",
    "    f1_relax = (2*pre_relax*rec_relax)/(pre_relax+rec_relax)\n",
    "    print(f\"Relax Level for {i}: Precision: {pre_relax}, Recall: {rec_relax}, F1: {f1_relax}\")\n",
    "    attbc_metrics[\"category\"][\"relax\"] = attbc_metrics[\"category\"].get(\"relax\", {})\n",
    "    attbc_metrics[\"category\"][\"relax\"][i] = {\"f_score\": f1_relax, \"precision\":pre_relax, \"recall\":rec_relax}\n",
    "\n",
    "    pre_strict = eval_metrics[\"prediction\"][i]['strict']/tp\n",
    "    rec_strict = eval_metrics[\"category\"][i]['strict']/tt\n",
    "    f1_strict = (2*pre_strict*rec_strict)/(pre_strict+rec_strict) if (pre_strict+rec_strict) != 0 else 0.0\n",
    "    print(f\"Strict Level for {i}: Precision: {pre_strict}, Recall: {rec_strict}, F1: {f1_strict}\")\n",
    "    print('\\n')\n",
    "    attbc_metrics[\"category\"][\"strict\"] = attbc_metrics[\"category\"].get(\"strict\", {})\n",
    "    attbc_metrics[\"category\"][\"strict\"][i] = {\"f_score\": f1_strict, \"precision\":pre_strict, \"recall\":rec_strict}\n",
    "json.dump(attbc_metrics, open(f\"{outfolder}/{outfile}.json\", \"w\", encoding=\"utf-8\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e994054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
