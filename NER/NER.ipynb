{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c179485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copyfile\n",
    "import csv\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6642f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7221b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = f\"chia/Chia_with_Scope\"\n",
    "outputpath = f\"chia/Chia_to_bio\"\n",
    "trainpath = f\"chia/Train\"\n",
    "testpath = f\"chia/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15dfd8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputfiles = set()\n",
    "for f in os.listdir(inputpath):\n",
    "    if f.endswith('.ann'):\n",
    "        inputfiles.add(f.split('.')[0].split('_')[0])\n",
    "len(inputfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "677b3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_types = ['Demographic', 'Condition', 'Value', 'Drug', 'Procedure', 'Measurement', 'Temporal', \\\n",
    "    'Observation', 'Person', 'Mood', 'Device', 'Pregnancy_considerations', 'Location', 'Negation', 'Qualifier', 'Multiplier', 'Reference_point', 'Line', 'Mood', 'Non-query-able', 'Post-eligibility',\n",
    "               'v-AND', 'Informed_consent', 'v-OR', 'Non-representable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3a60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Brat format into BIO format\n",
    "# function for getting entity annotations from the annotation file\n",
    "def get_annotation_entities(ann_file, select_types=None):\n",
    "    entities = []\n",
    "    with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                term = line.strip().split('\\t')[1].split()\n",
    "                if (select_types != None) and (term[0] not in select_types): continue\n",
    "                if int(term[-1]) <= int(term[1]): continue\n",
    "                entities.append((int(term[1]), int(term[-1]), term[0]))\n",
    "    return sorted(entities, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "# function for handling overlap by keeping the entity with largest text span\n",
    "def remove_overlap_entities(sorted_entities):\n",
    "    keep_entities = []\n",
    "    for idx, entity in enumerate(sorted_entities):\n",
    "        if idx == 0:\n",
    "            keep_entities.append(entity)\n",
    "            last_keep = entity\n",
    "            continue\n",
    "        if entity[0] < last_keep[1]:\n",
    "            if entity[1]-entity[0] > last_keep[1]-last_keep[0]:\n",
    "                last_keep = entity\n",
    "                keep_entities[-1] = last_keep\n",
    "        elif entity[0] == last_keep[1]:\n",
    "            last_keep = (last_keep[0], entity[1], last_keep[-1])\n",
    "            keep_entities[-1] = last_keep\n",
    "        else:\n",
    "            last_keep = entity\n",
    "            keep_entities.append(entity)\n",
    "    return keep_entities\n",
    "\n",
    "# inverse index of entity annotations\n",
    "def entity_dictionary(keep_entities, txt_file):\n",
    "    f_ann = {}\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f: # Marked\n",
    "        text = f.readlines()\n",
    "        text = '  '.join([i.strip() for i in text])\n",
    "    for entity in keep_entities:\n",
    "        entity_text = text[entity[0]:entity[1]]\n",
    "        doc = nlp(entity_text)\n",
    "        token_starts = [(i, doc[i:].start_char) for i in range(len(doc))]\n",
    "        term_type = entity[-1]\n",
    "        term_offset = entity[0]\n",
    "        for i, token in enumerate(doc):\n",
    "            ann_offset = token_starts[i][1]+term_offset\n",
    "            if ann_offset not in f_ann:\n",
    "                f_ann[ann_offset] = [i, token.text, term_type]\n",
    "    return f_ann\n",
    "\n",
    "# Brat -> BIO format conversion\n",
    "for infile in inputfiles:\n",
    "    for t in [\"exc\", \"inc\"]:\n",
    "        file = f\"{infile}_{t}\"\n",
    "        ann_file = f\"{inputpath}/{file}.ann\"\n",
    "        txt_file = f\"{inputpath}/{file}.txt\"\n",
    "        out_file = f\"{outputpath}/{file}.bio.txt\"\n",
    "        sorted_entities = get_annotation_entities(ann_file, select_types)\n",
    "        keep_entities = remove_overlap_entities(sorted_entities)\n",
    "        f_ann = entity_dictionary(keep_entities, txt_file)\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                sent_offset = 0\n",
    "                for line in f:\n",
    "                    # print(line.strip())\n",
    "                    if '⁄' in line:\n",
    "                        # print(txt_file)\n",
    "                        line = line.replace('⁄', '/') # replace non unicode characters\n",
    "                    doc = nlp(line.strip())\n",
    "                    token_starts = [(i, doc[i:].start_char) for i in range(len(doc))]\n",
    "                    for token in doc:\n",
    "                        token_sent_offset = token_starts[token.i][1]\n",
    "                        token_doc_offset = token_starts[token.i][1]+sent_offset\n",
    "                        if token_doc_offset in f_ann:\n",
    "                            if f_ann[token_doc_offset][0] == 0:\n",
    "                                label = f\"B-{f_ann[token_doc_offset][2]}\"\n",
    "                            else:\n",
    "                                label = f\"I-{f_ann[token_doc_offset][2]}\"\n",
    "                        else:\n",
    "                            label = f\"O\"\n",
    "                        f_out.write(f\"{token.text} {token_sent_offset} {token_sent_offset+len(token.text)} {token_doc_offset} {token_doc_offset+len(token.text)} {label}\\n\")\n",
    "                    f_out.write('\\n')\n",
    "\n",
    "                    #else:\n",
    "                    sent_offset += (len(line.strip())+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ddd994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 100 100\n"
     ]
    }
   ],
   "source": [
    "# dataset separation: 800 trials (80%) for training, 100 trials (10%) for validation and 100 trials (10%) for testing\n",
    "train_ids, dev_ids = train_test_split(list(inputfiles), train_size=0.8, random_state=13, shuffle=True)\n",
    "dev_ids, test_ids = train_test_split(dev_ids, train_size=0.5, random_state=13, shuffle=True)\n",
    "print(len(train_ids), len(dev_ids), len(test_ids))\n",
    "chia_datasets = {\"train\":train_ids, \"dev\":dev_ids, \"test\":test_ids}\n",
    "json.dump(chia_datasets, open(\"chia/chia_datasets.json\", \"w\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf8b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge BIO format train, validation and test datasets\n",
    "# chia_datasets = json.load(open(\"chia/chia_datasets.json\", \"r\", encoding=\"utf-8\"))\n",
    "# merge the train dataset\n",
    "with open(\"chia/train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"train\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{trainpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{trainpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# merge the validation dataset\n",
    "with open(\"chia/dev.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"dev\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{trainpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{trainpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "# merge the test dataset\n",
    "with open(\"chia/test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for fid in chia_datasets[\"test\"]:\n",
    "        copyfile(f\"{outputpath}/{fid}_exc.bio.txt\", f\"{testpath}/{fid}_exc.bio.txt\")\n",
    "        copyfile(f\"{outputpath}/{fid}_inc.bio.txt\", f\"{testpath}/{fid}_inc.bio.txt\")\n",
    "        with open(f\"{outputpath}/{fid}_exc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")\n",
    "        with open(f\"{outputpath}/{fid}_inc.bio.txt\", \"r\", encoding=\"utf-8\") as fr:\n",
    "            txt = fr.read().strip()\n",
    "            if txt != '':\n",
    "                f.write(txt)\n",
    "                f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5e84d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Chia in Brat into format for Att-BiLSTM-CRF model\n",
    "out_file = f\"chia/chia_ner.tsv\"\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for infile in inputfiles:\n",
    "        for t in [\"exc\", \"inc\"]:\n",
    "            file = f\"{infile}_{t}\"\n",
    "            ann_file = f\"{inputpath}/{file}.ann\"\n",
    "            txt_file = f\"{inputpath}/{file}.txt\"\n",
    "            sorted_entities = get_annotation_entities(ann_file, select_types)\n",
    "            keep_entities = remove_overlap_entities(sorted_entities)\n",
    "            with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                sent_offset = 0\n",
    "                for line in f:\n",
    "                    # print(line.strip())\n",
    "                    if '⁄' in line: line = line.replace('⁄', '/')\n",
    "                    sent_end = sent_offset + len(line)\n",
    "                    sent_ents = []\n",
    "                    for ent in keep_entities:\n",
    "                        if ent[0] < sent_offset or ent[1] < sent_offset: continue\n",
    "                        if ent[0] >= sent_end or ent[1] > sent_offset+len(line.strip()): break\n",
    "                        ent_start = ent[0]-sent_offset+1\n",
    "                        ent_end = ent[1]-sent_offset+1\n",
    "                        sent_ents.append(f\"{ent_start}:{ent_end}:{ent[2].lower()}\")\n",
    "                    if sent_ents == []:\n",
    "                        if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']:\n",
    "                            sent_offset += (len(line.strip())+1)\n",
    "                        else:\n",
    "                            sent_offset += (len(line.strip())+2)\n",
    "                        continue\n",
    "                    # print(f\"{file}\\t{','.join(sent_ents)}\\t{line.strip()}\")\n",
    "                    f_out.write(f\"{file}\\t{','.join(sent_ents)}\\t{line.strip()}\")\n",
    "                    # print('\\n')\n",
    "                    f_out.write('\\n')\n",
    "                    if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']:\n",
    "                        sent_offset += (len(line.strip())+1)\n",
    "                    else:\n",
    "                        sent_offset += (len(line.strip())+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46fbd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split Chia in format for Att-BiLSTM-CRF model into train, validation and test datasets\n",
    "# chia_datasets = json.load(open(\"chia/chia_datasets.json\", \"r\", encoding=\"utf-8\"))\n",
    "with open(\"chia/chia_ner_train.tsv\", \"w\", encoding=\"utf-8\") as ftrain, open(\"chia/chia_ner_dev.tsv\", \"w\", encoding=\"utf-8\") as fdev, open(\"chia/chia_ner_test.tsv\", \"w\", encoding=\"utf-8\") as ftest:\n",
    "    with open(\"chia/chia_ner.tsv\", \"r\", encoding=\"utf-8\") as fread:\n",
    "        for line in fread:\n",
    "            if line.split('\\t', 1)[0].split(\"_\")[0] in chia_datasets[\"train\"]:\n",
    "                ftrain.write(line)\n",
    "            elif line.split('\\t', 1)[0].split(\"_\")[0] in chia_datasets[\"dev\"]:\n",
    "                fdev.write(line)\n",
    "            else:\n",
    "                ftest.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b93e6fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda94a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformer used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02ae006d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e356102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 'chia' \n",
    "# outfolder = f\"transformer\"\n",
    "# test_files = os.listdir(f\"{dataset}/tests/\")\n",
    "# models = ['bert','albert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418f087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
